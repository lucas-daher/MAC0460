{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset #\n",
    "\n",
    "Famoso conjunto de imagens de dígidos manuscritos, dividido em $60000$ imagens de treinamento e $10000$ imagens de teste. Todas as imagens são em escala de cinza com tamanho $28 \\times 28$. Existem 10 classes, correspondentes aos dígitos de 0 a 9.\n",
    "\n",
    "Este dataset está disponível em diversos lugares. Abaixo, vamos usar a versão disponível na biblioteca Keras [1]. Informações adicionais estão disponíveis no [site oficial](http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos visualizar algumas das imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize = (18, 12))\n",
    "\n",
    "for i in range(6):\n",
    "    ax[i//3, i%3].imshow(x_train[i], cmap='gray')\n",
    "    ax[i//3, i%3].axis('off')\n",
    "    print('label:', y_train[i])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de Features #\n",
    "\n",
    "Por enquanto, não iremos aplicar nenhuma técnica de aprendizado diretamente sobre as imagens (pixels). Vamos utilizar alguns atributos que iremos extrair a seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simetria ###\n",
    "\n",
    "No livro _Learning from Data_ [2], um dos atributos que os autores extraem como exemplo do MNIST é a Simetria horizontal.\n",
    "\n",
    "Seja a assimetria definida como a diferença absoluta média entre os pixels da imagem original e da imagem refletida horizontalmente, a simetria é simplesmente o inverso da assimetria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simetry(image):\n",
    "    # A operação abaixo inverte a ordem das colunas da imagem\n",
    "    reflected_image = image[:, ::-1]\n",
    "    return -np.mean(np.abs(image - reflected_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensidade Média da Imagem ###\n",
    "\n",
    "Outro atributo extraído como exemplo no livro _Learning from Data_ é a intensidade média dos pixels. Este atributo está associado a proporção da imagem ocupada pelo dígito. Por exemplo, a intensidade de uma imagem com o dígito $1$ é menor do que a de uma imagem com o dígito $2$ out $5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def average_intensity(image):\n",
    "    return np.mean(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixels $\\rightarrow$ Features ##\n",
    "\n",
    "Agora, aplicamos as funções descritas anteriormente sobre as imagens do MNIST. Note que, na forma de imagens, os dados eram representados por $28 \\times 28 = 784$ atributos, enquanto que agora, eles são representados por apenas $2$ atributos.\n",
    "\n",
    "É de se esperar que um certo grau de informação se perca neste mapeamento. Em situações reais, escolhemos as features a serem extraídas de modo a preservar o máximo possível de informações úteis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A função abaixo converte uma imagem em uma lista de atributos,\n",
    "# usando as funções definidas acima\n",
    "def convert2features(image):\n",
    "    return np.array([average_intensity(image), simetry(image)])\n",
    "\n",
    "# Aplica a conversão a todas as entradas do dataset\n",
    "x_train_features = np.array([convert2features(image) for image in x_train])\n",
    "x_test_features  = np.array([convert2features(image) for image in x_test])\n",
    "\n",
    "# Ajusta a escala das features. Utilizar multiplas features com escalas diferentes pode ser problemático\n",
    "for i in range(x_train_features.shape[1]):\n",
    "    avg = np.mean(x_train_features[:, i])\n",
    "    stddev = np.std(x_train_features[:, i])\n",
    "    x_train_features[:, i] = (x_train_features[:, i] - avg) / stddev\n",
    "    # (Sim, as features no conjunto de teste são ajustadas usando as estatísticas do conjunto de treinamento)\n",
    "    x_test_features[:, i] = (x_test_features[:, i] - avg) / stddev\n",
    "\n",
    "print(x_train_features.shape)\n",
    "print(x_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos #\n",
    "\n",
    "Da mesma forma que o exemplo do livro, vamos nos concentrar em identificar apenas os dígitos $1$ e $5$. Vamos juntar os dados com labels 1 e 5, e embaralhar a ordem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antes de mais nada, definir a seed aleatória como uma constante,\n",
    "# de forma que todos os experimentos obtenham o mesmo resultado\n",
    "np.random.seed(56789)\n",
    "\n",
    "x_train_1 = x_train_features[y_train == 1]\n",
    "x_train_5 = x_train_features[y_train == 5]\n",
    "y_train_1 = y_train[y_train == 1]\n",
    "y_train_5 = y_train[y_train == 5]\n",
    "\n",
    "x_test_1 = x_test_features[y_test == 1]\n",
    "x_test_5 = x_test_features[y_test == 5]\n",
    "y_test_1 = y_test[y_test == 1]\n",
    "y_test_5 = y_test[y_test == 5]\n",
    "\n",
    "x_train_features_1_5 = np.concatenate([x_train_1, x_train_5], axis = 0)\n",
    "y_train_1_5 = np.concatenate([y_train_1, y_train_5], axis = 0).astype('float32')\n",
    "\n",
    "x_test_features_1_5 = np.concatenate([x_test_1, x_test_5], axis = 0)\n",
    "y_test_1_5 = np.concatenate([y_test_1, y_test_5], axis = 0).astype('float32')\n",
    "\n",
    "# Considere o digito 1 como a instancia negativa (-1) e o 5 como a positiva (+1)\n",
    "y_train_1_5[y_train_1_5 == 1] = -1\n",
    "y_train_1_5[y_train_1_5 == 5] = +1\n",
    "\n",
    "y_test_1_5[y_test_1_5 == 1] = -1\n",
    "y_test_1_5[y_test_1_5 == 5] = +1\n",
    "\n",
    "def shuffle(X, y):\n",
    "    # Com cuidado para embaralhar a entrada e a saída da mesma forma\n",
    "    perm = np.random.permutation(len(X))\n",
    "    return X[perm], y[perm]\n",
    "\n",
    "x_train_features_1_5, y_train_1_5 = shuffle(x_train_features_1_5, y_train_1_5)\n",
    "\n",
    "print(x_train_features_1_5.shape, y_train_1_5.shape)\n",
    "print(x_test_features_1_5.shape, y_test_1_5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, vamos plotar o conjunto de treinamento no plano cartesiano, onde os eixos correspondem aos atributos que acabamos de extrair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "# Plotamos os 1s em azul...\n",
    "plt.scatter(x = x_train_1[:,0], y = x_train_1[:,1], c = 'blue', alpha = 0.4)\n",
    "\n",
    "# ...e os 5s em verde\n",
    "plt.scatter(x = x_train_5[:,0], y = x_train_5[:,1], c = 'green', alpha = 0.4)\n",
    "\n",
    "plt.xlabel('Intensidade Média')\n",
    "plt.ylabel('Simetria')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de se esperar, os $1$s geralmente possuem menor intensidade maior simetria que os $5$s. Além disso, existe mais variação na forma como é possível desenhar os $5$s (note a dispersão dos pontos verdes).\n",
    "\n",
    "É importante notar que existe uma sobreposição considerável de $1$s e $5$s, indicando a não separabilidade linear dos dados quando representado através destas features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron ##\n",
    "\n",
    "O primeiro algoritmo avaliado será a versão básica do Perceptron. Iremos rodar o PLA por 1000 iterações, sempre escolhendo o primeiro exemplo erroneamente classificado do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna a saida do perceptron definido pelo vetor de pesos w (shape = (d,))\n",
    "# aplicado aos exemplos na matriz X (shape = (N, d))\n",
    "def predict(X, w):\n",
    "    # Make sure the data matrix has a bias coordinate\n",
    "    if X.shape[1] != w.shape[0]:\n",
    "        # Add a bias value 1 as the first coordinate of each vector\n",
    "        X = np.concatenate([np.ones((len(X), 1)), X], axis = 1)\n",
    "    return np.sign(np.dot(X, w))\n",
    "\n",
    "# A função a seguir recebe a matriz de dados X (shape = (N, d)) e vetor de saída\n",
    "# y (shape = (N,)), e retorna um vetor de pesos de acordo com o PLA\n",
    "# e, caso return_history = True, uma lista com a quantidade de erros cometidos a cada iteração\n",
    "def PLA(X, y, w0 = None, max_iterations = 1000, return_history = False):\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "np.random.seed(1257)\n",
    "w_pla, errors = PLA(x_train_features_1_5, y_train_1_5, return_history = True)\n",
    "\n",
    "print(w_pla)\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.plot(errors)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Iteration #')\n",
    "plt.ylabel('Misclassified Samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que a quantidade de erros oscila significativamente ao longo das iterações. Isto se deve a natureza local das atualizações realizadas pelo PLA. Ao corrigir a classificação de um ponto, outros pontos (possivelmente vários) corretos podem passar para o lado errado da fronteira de classificação.\n",
    "\n",
    "Vamos visualizar a fronteira de classificação obtida junto com os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "# Plotamos os 1s em azul...\n",
    "plt.scatter(x = x_train_1[:,0], y = x_train_1[:,1], c = 'blue', alpha = 0.4)\n",
    "\n",
    "# ...e os 5s em verde\n",
    "plt.scatter(x = x_train_5[:,0], y = x_train_5[:,1], c = 'green', alpha = 0.4)\n",
    "\n",
    "# A fronteira é uma linha no formato w_pla[0] + w_pla[1]*intensidade + w_pla[2]*simetria = 0\n",
    "# Obter dois pontos quaisquer na fronteira\n",
    "\n",
    "p1 = (-2, -(w_pla[0] - 2*w_pla[1])/w_pla[2])\n",
    "p2 = (2,  -(w_pla[0] + 2*w_pla[1])/w_pla[2])\n",
    "\n",
    "lines = plt.plot([p1[0], p2[0]], [p1[1], p2[1]], '-')\n",
    "plt.setp(lines, color='r', linewidth=4.0)\n",
    "\n",
    "plt.xlabel('Intensidade Média')\n",
    "plt.ylabel('Simetria')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de confusão ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "y_true = y_train_1_5\n",
    "y_pred = predict(x_train_features_1_5, w_pla)\n",
    "\n",
    "true_positives = np.sum((y_pred == +1) * (y_true == +1))\n",
    "true_negatives = np.sum((y_pred == -1) * (y_true == -1))\n",
    "\n",
    "false_positives = np.sum((y_pred == +1) * (y_true == -1))\n",
    "false_negatives = np.sum((y_pred == -1) * (y_true == +1))\n",
    "\n",
    "confusion = [\n",
    "    [true_positives, false_positives],\n",
    "    [false_negatives, true_negatives]\n",
    "]\n",
    "\n",
    "df_cm = pd.DataFrame(confusion, ['$\\hat{y} = 5$', '$\\hat{y} = 1$'], ['$y = 5$', '$y = 1$'])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como já era de se esperar pela fronteira de classificação, os $1$s foram classificados mais facilmente do que os $5$s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pocket ##\n",
    "\n",
    "Desta vez, vamos usar a versão pocket do Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função a seguir recebe a matriz de dados X (shape = (N, d)) e vetor de saída\n",
    "# y (shape = (N,)), e retorna um vetor de pesos de acordo com o Pocket Perceptron\n",
    "# e, caso return_history = True, uma lista com a quantidade de erros cometidos a cada iteração\n",
    "def PLA_pocket(X, y, w0 = None, max_iterations = 1000, return_history = False):\n",
    "    raise NotImplementedError\n",
    "\n",
    "np.random.seed(18375)\n",
    "w_pocket, errors = PLA_pocket(x_train_features_1_5, y_train_1_5, return_history = True)\n",
    "\n",
    "print(w_pocket)\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.plot(errors)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iteration #')\n",
    "plt.ylabel('Misclassified Samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era esperado, a quantidade de erros associados ao vetor de pesos que está \"no pocket\" jamais aumenta.\n",
    "\n",
    "Vamos visualizar a nova fronteira de classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "# Plotamos os 1s em azul...\n",
    "plt.scatter(x = x_train_1[:,0], y = x_train_1[:,1], c = 'blue', alpha = 0.4)\n",
    "\n",
    "# ...e os 5s em verde\n",
    "plt.scatter(x = x_train_5[:,0], y = x_train_5[:,1], c = 'green', alpha = 0.4)\n",
    "\n",
    "# A fronteira é uma linha no formato w_pla[0] + w_pla[1]*intensidade + w_pla[2]*simetria = 0\n",
    "# Obter dois pontos quaisquer na fronteira\n",
    "\n",
    "p1 = (-2, -(w_pocket[0] - 2*w_pocket[1])/w_pocket[2])\n",
    "p2 = (1,  -(w_pocket[0] + 1*w_pocket[1])/w_pocket[2])\n",
    "\n",
    "lines = plt.plot([p1[0], p2[0]], [p1[1], p2[1]], '-')\n",
    "plt.setp(lines, color='r', linewidth=4.0)\n",
    "\n",
    "plt.xlabel('Intensidade Média')\n",
    "plt.ylabel('Simetria')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de confusão ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "y_true = y_train_1_5\n",
    "y_pred = predict(x_train_features_1_5, w_pocket)\n",
    "\n",
    "true_positives = np.sum((y_pred == +1) * (y_true == +1))\n",
    "true_negatives = np.sum((y_pred == -1) * (y_true == -1))\n",
    "\n",
    "false_positives = np.sum((y_pred == +1) * (y_true == -1))\n",
    "false_negatives = np.sum((y_pred == -1) * (y_true == +1))\n",
    "\n",
    "confusion = [\n",
    "    [true_positives, false_positives],\n",
    "    [false_negatives, true_negatives]\n",
    "]\n",
    "\n",
    "df_cm = pd.DataFrame(confusion, ['$\\hat{y} = 5$', '$\\hat{y} = 1$'], ['$y = 5$', '$y = 1$'])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinal da Regressão Linear ##\n",
    "\n",
    "Desta vez, vamos usar o sinal da regressão linear para classificar os exemplos. \n",
    "\n",
    "Queremos minimizar o erro quadrado médio:\n",
    "\n",
    "$$E_{in}(\\mathbf{w}) = \\frac{1}{N} \\sum_{n=1}^N (\\mathbf{w}^T \\mathbf{x}_n - y_n)^2$$\n",
    "\n",
    "Para o qual existe uma fórmula fechada (demonstração no livro _Learning from Data_ [2]):\n",
    "\n",
    "$$\\mathbf{w}_{lin} = \\text{X}^{\\dagger} y$$\n",
    "\n",
    "onde\n",
    "\n",
    "$$\\text{X}^{\\dagger} = (\\text{X}^T \\text{X})^{-1} \\text{X}^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função a seguir recebe a matriz de dados X (shape = (N, d)) e vetor de saída\n",
    "# y (shape = (N,)), e retorna o (único) vetor de pesos que minimiza o erro quadrado médio.\n",
    "def linear_regression(X, y):\n",
    "    raise NotImplementedError\n",
    "    \n",
    "w_lin = linear_regression(x_train_features_1_5, y_train_1_5)\n",
    "\n",
    "print(w_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "# Plotamos os 1s em azul...\n",
    "plt.scatter(x = x_train_1[:,0], y = x_train_1[:,1], c = 'blue', alpha = 0.4)\n",
    "\n",
    "# ...e os 5s em verde\n",
    "plt.scatter(x = x_train_5[:,0], y = x_train_5[:,1], c = 'green', alpha = 0.4)\n",
    "\n",
    "# A fronteira é uma linha no formato w_lin[0] + w_lin[1]*intensidade + w_lin[2]*simetria = 0\n",
    "# Obter dois pontos quaisquer na fronteira\n",
    "\n",
    "p1 = (-2, -(w_lin[0] - 2*w_lin[1])/w_lin[2])\n",
    "p2 = (1,  -(w_lin[0] + 1*w_lin[1])/w_lin[2])\n",
    "\n",
    "lines = plt.plot([p1[0], p2[0]], [p1[1], p2[1]], '-')\n",
    "plt.setp(lines, color='r', linewidth=4.0)\n",
    "\n",
    "plt.xlabel('Intensidade Média')\n",
    "plt.ylabel('Simetria')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível perceber que a fronteira obtida com a regressão linear faz sentido em relação as classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de confusão ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "y_true = y_train_1_5\n",
    "y_pred = predict(x_train_features_1_5, w_lin)\n",
    "\n",
    "true_positives = np.sum((y_pred == +1) * (y_true == +1))\n",
    "true_negatives = np.sum((y_pred == -1) * (y_true == -1))\n",
    "\n",
    "false_positives = np.sum((y_pred == +1) * (y_true == -1))\n",
    "false_negatives = np.sum((y_pred == -1) * (y_true == +1))\n",
    "\n",
    "confusion = [\n",
    "    [true_positives, false_positives],\n",
    "    [false_negatives, true_negatives]\n",
    "]\n",
    "\n",
    "df_cm = pd.DataFrame(confusion, ['$\\hat{y} = 5$', '$\\hat{y} = 1$'], ['$y = 5$', '$y = 1$'])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão Logística ##\n",
    "\n",
    "Neste próximo exercício, vamos tentar aplicar regressão logística. Antes de mais nada, vamos implementar a função de predição, que faz uso da função sigmoid, definida abaixo:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Retorna a saida da regressão logística definida pelo vetor de pesos w (shape = (d,))\n",
    "# aplicado aos exemplos na matriz X (shape = (N, d))\n",
    "def predict_logistic(X, w):\n",
    "    # Make sure the data matrix has a bias coordinate\n",
    "    if X.shape[1] != w.shape[0]:\n",
    "        # Add a bias value 1 as the first coordinate of each vector\n",
    "        X = np.concatenate([np.ones((len(X), 1)), X], axis = 1)\n",
    "    return sigmoid(np.dot(X, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos implementar nossa função de custo. No nosso caso, entropia cruzada.\n",
    "\n",
    "$$E_{in}(\\mathbf{w}) = \\frac{1}{N} \\sum_{n=1}^{N} \\ln(1 + e^{-y_n \\mathbf{w}^T \\mathbf{x}_n})$$\n",
    "\n",
    "E o seu gradiente, com relação aos pesos. Sabemos que o gradiente de $E_{in}$ é da seguinte forma:\n",
    "\n",
    "$$\\nabla E_{in}(\\mathbf{w}) = - \\frac{1}{N}\\sum_{n=1}^{N} \\frac{y_n \\mathbf{x}_n}{1 + e^{y_n \\mathbf{w}^T \\mathbf{x}_n}}$$\n",
    "\n",
    "As funções a seguir utilizam várias operações vetoriais em numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(w, X, y):\n",
    "    return np.mean(np.log(1 + np.exp(-y * np.dot(X, w))))\n",
    "\n",
    "def cross_entropy_gradient(w, X, y):\n",
    "    N = X.shape[0]\n",
    "    return -np.dot(X.transpose(), y / (1 + np.exp(y * np.dot(X, w)))) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, o treinamento do modelo de regressão logística, usando _gradient descent_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função a seguir recebe a matriz de dados X (shape = (N, d)) e vetor de saída\n",
    "# y (shape = (N,)), e retorna um vetor de pesos de acordo com o treinamento da regressão logística\n",
    "# e, caso return_history = True, uma lista com o valor de cross_entropy a cada iteração\n",
    "def logistic_regression(X, y, batch_size = None, learning_rate = 1e-2, w0 = None, num_iterations = 1000, return_history = False):\n",
    "    raise NotImplementedError\n",
    "\n",
    "np.random.seed(56789)\n",
    "w_logistic, loss = logistic_regression(x_train_features_1_5, y_train_1_5, num_iterations = 20000, return_history = True)\n",
    "\n",
    "print(w_logistic)\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.plot(loss)\n",
    "plt.xlabel('Iteration #')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como é possível perceber pelo gráfico, o custo aparenta ter estagnado a partir de 10000 iterações. Como a regressão logística produz uma probabilidade, ao invés de visualizar a fronteira de classificação, vamos visualizar a intensidade da saída.\n",
    "\n",
    "No gráfico abaixo, a intensidade de vermelho corresponde a probabilidade atribuída pela regressão logística para cada ponto. Quanto mais próximo de 1, maior é a confiança da regressão de que o ponto em questão seja um $5$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "y_pred = predict_logistic(x_train_features_1_5, w_logistic)\n",
    "\n",
    "plt.scatter(x = x_train_features_1_5[:,0], y = x_train_features_1_5[:,1], c = y_pred, cmap = 'coolwarm')\n",
    "\n",
    "plt.xlabel('Intensidade Média')\n",
    "plt.ylabel('Simetria')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como é possível perceber, pontos nas extremidades do cluster possuem baixa probabilidade (azul escuro, maior confiança de ser um $1$), ou alta (vermelho escuro, maior confiança de ser um $5$), enquanto pontos na intersecção das duas classes possuem uma intensidade intermediária, associada com uma saída próxima de 0.5, idicando que a regressão não diferencia com confiança as classes nesta região."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de confusão ###\n",
    "\n",
    "Para atribuírmos categorias para a saída da regressão logística, precisamos definir um threshold entre 0 e 1, que diferencie as saídas positivas das negativas.\n",
    "\n",
    "Em situações reais, este threshold deve ser definido de acordo com o custo associado a cada tipo de erro (falso positivo vs falso negativo, lembre-se do exemplo do supermercado e da CIA). Nesta situação, vamos manter o raciocínio simples e assumir um threshold de 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "y_true = y_train_1_5\n",
    "y_pred = predict_logistic(x_train_features_1_5, w_logistic) > threshold\n",
    "\n",
    "true_positives = np.sum(y_pred * (y_true == +1))\n",
    "true_negatives = np.sum((1-y_pred) * (y_true == -1))\n",
    "\n",
    "false_positives = np.sum(y_pred * (y_true == -1))\n",
    "false_negatives = np.sum((1-y_pred) * (y_true == +1))\n",
    "\n",
    "confusion = [\n",
    "    [true_positives, false_positives],\n",
    "    [false_negatives, true_negatives]\n",
    "]\n",
    "\n",
    "df_cm = pd.DataFrame(confusion, ['$\\hat{y} = 5$', '$\\hat{y} = 1$'], ['$y = 5$', '$y = 1$'])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]: François Chollet and others, Keras, https://keras.io, 2015\n",
    "\n",
    "[2]: Yaser S Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin, _Learning from Data_, 2012"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
